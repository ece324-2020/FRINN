{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "multihead_conv_net_FINAL.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1n9d60rhiYBs"
      },
      "source": [
        "#**Import and Mount and GPU**\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P_b09jRDiX0v"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "import numpy as np                                              \n",
        "import pandas as pd                                             \n",
        "import matplotlib.pyplot as plt                                 \n",
        "import torchvision                \n",
        "import torchvision.transforms as transforms              \n",
        "from torch.utils.data import SubsetRandomSampler \n",
        "import torch.utils.data as data   \n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torchsummary import summary\n",
        "import torch.nn.functional as F\n",
        "import sys\n",
        "from time import time\n",
        "sys.path.append('/content/gdrive/MyDrive/ECE324 project/Additional files/Data_Loader.py')\n",
        "sys.path.append('/content/gdrive/My Drive/ECE324 project/Additional files')\n",
        "sys.path.append('/content/gdrive/My Drive/ECE324 project/Clean Data')\n",
        "sys.path.append('/content/gdrive/MyDrive/ECE324 project/test_varied_bg')\n",
        "sys.path.append('/content/gdrive/MyDrive/ECE324 project/test_white_bg')\n",
        "from Data_Loader import *\n",
        "from sklearn.metrics import confusion_matrix, classification_report\n",
        "from torchvision import models\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g1iLBfXmWnw5"
      },
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-1MId-kYaGLE"
      },
      "source": [
        "#**Loading in the Data**\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vy1PW1kSaGsE"
      },
      "source": [
        "\n",
        "my_transform = transforms.Compose([\n",
        "  transforms.ToTensor(),\n",
        "  transforms.Resize((224,224)),\n",
        "  transforms.Normalize(mean = [0.66445047,0.55465436,0.447036], std = [0.321551,0.33547384,0.3524585])\n",
        "  ])\n",
        "\n",
        "dataset =  torchvision.datasets.ImageFolder(root='/content/drive/MyDrive/ECE324 project/Clean Data', transform=my_transform)\n",
        "print(len(dataset))\n",
        "train_data, valid_data, test_data = torch.utils.data.random_split(dataset, [9600,2400,3000], generator=torch.Generator().manual_seed(0))\n",
        "\n",
        "train_loader = DataLoader(train_data, batch_size=64,shuffle= True)                  \n",
        "valid_loader = DataLoader(valid_data, batch_size=64,shuffle= True)\n",
        "test_loader = DataLoader(test_data, batch_size=64,shuffle= True)\n",
        "\n",
        "def relabel_state(argument):\n",
        "    switcher = {\n",
        "        0: 0,\n",
        "        1: 0,\n",
        "        2: 0,\n",
        "        3: 0,\n",
        "        4: 0,\n",
        "        5: 1,\n",
        "        6: 1,\n",
        "        7: 1,\n",
        "        8: 1,\n",
        "        9: 1,\n",
        "        10:2,\n",
        "        11:2,\n",
        "        12:2,\n",
        "        13:2,\n",
        "        14:2}\n",
        "    return switcher[argument.item()]\n",
        "\n",
        "def relabel_type(argument):\n",
        "    switcher = {\n",
        "        0: 0,\n",
        "        1: 1,\n",
        "        2: 2,\n",
        "        3: 3,\n",
        "        4: 4,\n",
        "        5: 0,\n",
        "        6: 1,\n",
        "        7: 2,\n",
        "        8: 3,\n",
        "        9: 4,\n",
        "        10:0,\n",
        "        11:1,\n",
        "        12:2,\n",
        "        13:3,\n",
        "        14:4}\n",
        "    return switcher[argument.item()]\n",
        "\n",
        "def recombine_prediction(argument1,argument2):\n",
        "    switcher = {\n",
        "         str([0,0]): 0,\n",
        "         str([0,1]): 1,\n",
        "         str([0,2]): 2,\n",
        "         str([0,3]): 3,\n",
        "         str([0,4]): 4,\n",
        "         str([1,0]): 5,\n",
        "         str([1,1]): 6,\n",
        "         str([1,2]): 7,\n",
        "         str([1,3]): 8,\n",
        "         str([1,4]): 9,\n",
        "         str([2,0]):10,\n",
        "         str([2,1]):11,\n",
        "         str([2,2]):12,\n",
        "         str([2,3]):13,\n",
        "         str([2,4]):14}\n",
        "    return switcher[str([argument1.item(),argument2.item()])]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kVEuQAMeZBRC"
      },
      "source": [
        "\n",
        "# **Various Multi head - CNN architectures**\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TaATluBy5dNc"
      },
      "source": [
        "# CNN Model\n",
        "\n",
        "# for 15,5--> 30,5 use 85720\n",
        "# for 30,5,30,5 --> 84270\n",
        "#30,10    30,10 --> 72030\n",
        "#30,10    50,10 -->120050\n",
        "class CNN(nn.Module):\n",
        "  def __init__(self):\n",
        "    super(CNN,self).__init__()\n",
        "    # Convolution layers\n",
        "    self.conv1 = nn.Conv2d(3,30,10) # 6 kernels, 5x5\n",
        "    self.pool = nn.MaxPool2d(2,2)\n",
        "    self.conv2 = nn.Conv2d(30,50,20) # 16 kernels, 5x5\n",
        "\n",
        "    # MLP State\n",
        "    self.fc1 = nn.Linear(96800,500)\n",
        "    self.fc2 = nn.Linear(500,120)\n",
        "    self.fc3 = nn.Linear(120,3)\n",
        "\n",
        "    # MLP Type\n",
        "    self.fc4 = nn.Linear(96800,300)#44944\n",
        "    self.fc5 = nn.Linear(300,100)\n",
        "    self.fc6 = nn.Linear(100,5)\n",
        "\n",
        "    #self.drop = nn.Dropout(p=0.5)\n",
        "    #self.drop_conv = nn.Dropout(p=0.1)\n",
        "    self.Bn1 = nn.BatchNorm2d(30)\n",
        "    self.Bn2 = nn.BatchNorm2d(50)\n",
        "\n",
        "  def forward(self,x):\n",
        "    x = self.pool(F.relu(self.Bn1(self.conv1(x))))\n",
        "    x = self.pool(F.relu(self.Bn2(self.conv2(x))))\n",
        "    #x = self.pool(F.relu(self.conv1(x)))\n",
        "    #x = self.pool(F.relu(self.conv2(x)))\n",
        "    x = x.view(x.size(0), -1)\n",
        "    #print(x.size())\n",
        "\n",
        "    # State head\n",
        "    x1 = F.relu(self.fc1(x))\n",
        "    x1 = F.relu(self.fc2(x1))\n",
        "    x1 = self.fc3(x1)\n",
        "\n",
        "    # Type head\n",
        "    x2 = F.relu(self.fc4(x))\n",
        "    x2 = F.relu(self.fc5(x2))\n",
        "    x2 = self.fc6(x2)\n",
        "\n",
        "    return x1, x2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0sQQUXgctG69"
      },
      "source": [
        "# **Early stopping code from github**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dBH3jJoiXx2A"
      },
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "\n",
        "class EarlyStopping:\n",
        "    \"\"\"Early stops the training if validation loss doesn't improve after a given patience.\"\"\"\n",
        "    def __init__(self, patience=7, verbose=False, delta=0, path='checkpoint.pt', trace_func=print):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            patience (int): How long to wait after last time validation loss improved.\n",
        "                            Default: 7\n",
        "            verbose (bool): If True, prints a message for each validation loss improvement. \n",
        "                            Default: False\n",
        "            delta (float): Minimum change in the monitored quantity to qualify as an improvement.\n",
        "                            Default: 0\n",
        "            path (str): Path for the checkpoint to be saved to.\n",
        "                            Default: 'checkpoint.pt'\n",
        "            trace_func (function): trace print function.\n",
        "                            Default: print            \n",
        "        \"\"\"\n",
        "        self.patience = patience\n",
        "        self.verbose = verbose\n",
        "        self.counter = 0\n",
        "        self.best_score = None\n",
        "        self.early_stop = False\n",
        "        self.val_loss_min = np.Inf\n",
        "        self.delta = delta\n",
        "        self.path = path\n",
        "        self.trace_func = trace_func\n",
        "    def __call__(self, val_loss, model):\n",
        "\n",
        "        score = -val_loss\n",
        "\n",
        "        if self.best_score is None:\n",
        "            self.best_score = score\n",
        "            self.save_checkpoint(val_loss, model)\n",
        "        elif score < self.best_score + self.delta:\n",
        "            self.counter += 1\n",
        "            self.trace_func(f'EarlyStopping counter: {self.counter} out of {self.patience}')\n",
        "            if self.counter >= self.patience:\n",
        "                self.early_stop = True\n",
        "        else:\n",
        "            self.best_score = score\n",
        "            self.save_checkpoint(val_loss, model)\n",
        "            self.counter = 0\n",
        "\n",
        "    def save_checkpoint(self, val_loss, model):\n",
        "        '''Saves model when validation loss decrease.'''\n",
        "        if self.verbose:\n",
        "            self.trace_func(f'Validation loss decreased ({self.val_loss_min:.6f} --> {val_loss:.6f}).  Saving model ...')\n",
        "        torch.save(model.state_dict(), self.path)\n",
        "        self.val_loss_min = val_loss"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vfzN0KARYsnr"
      },
      "source": [
        "#**Instantiate arguments and parameters**\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-mUAxODs2LQi"
      },
      "source": [
        "#MODEL ARGUMENTS\n",
        "lr = 0.03\n",
        "epochs = 50\n",
        "eval_every = 10\n",
        "model = CNN()\n",
        "\n",
        "model.to(device)\n",
        "\n",
        "\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=lr)\n",
        "loss_fnc= torch.nn.CrossEntropyLoss()\n",
        "\n",
        "loss_fnc.to(device)\n",
        "\n",
        "\n",
        "#OUTPUT DATA ARGUMENTS\n",
        "seed = 1\n",
        "torch.manual_seed(seed)\n",
        "save= True\n",
        "confusion=True\n",
        "plot = True\n",
        "statistics = True"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P7J53Vdz68Nb"
      },
      "source": [
        "### **Training loop using train and validation**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1RzcbvXXf_pP"
      },
      "source": [
        "#Prepare plotting lists\n",
        "train_loss_list = []\n",
        "valid_loss_list = []\n",
        "test_loss_list = []\n",
        "train_acc_list = []\n",
        "valid_acc_list = []\n",
        "test_acc_list = []\n",
        "\n",
        "train_loss_list_state = []\n",
        "valid_loss_list_state = []\n",
        "test_loss_list_state = []\n",
        "train_acc_list_state = []\n",
        "valid_acc_list_state = []\n",
        "test_acc_list_state = []\n",
        "\n",
        "train_loss_list_type = []\n",
        "valid_loss_list_type = []\n",
        "test_loss_list_type = []\n",
        "train_acc_list_type = []\n",
        "valid_acc_list_type = []\n",
        "test_acc_list_type = []\n",
        "\n",
        "\n",
        "epoch_list = []         #Epoch number\n",
        "start = time()          #Time training process\n",
        "\n",
        "for e in range(epochs):\n",
        "    running_loss = []\n",
        "    running_accuracy = []\n",
        "    running_loss_state = []\n",
        "    running_accuracy_state = []\n",
        "    running_loss_type = []\n",
        "    running_accuracy_type = []\n",
        "\n",
        "\n",
        "    running_valid_loss = []\n",
        "    running_valid_accuracy = []\n",
        "    running_valid_loss_state = []\n",
        "    running_valid_accuracy_state = []\n",
        "    running_valid_loss_type = []\n",
        "    running_valid_accuracy_type = []\n",
        "    print(\"going thru data , epoch {}\".format(e))\n",
        "  #               LOOKING AT TRAINING DATA\n",
        "\n",
        "    for i, data in enumerate(train_loader):\n",
        "\n",
        "        inputs, label = data[0].to(device),data[1].to(device)\n",
        "\n",
        "        label_state = torch.zeros(len(label)).to(device)\n",
        "        label_type = torch.zeros(len(label)).to(device)\n",
        "\n",
        "        for k in range(len(label)):\n",
        "          label_state[k] = relabel_state(label[k])\n",
        "          label_type[k] = relabel_type(label[k])\n",
        "\n",
        "            #setting gradients to zero and running model on batch\n",
        "        optimizer.zero_grad()\n",
        "        predictions_state, predictions_type = model(inputs)\n",
        "\n",
        "            #computing loss based on Cross Entropy\n",
        "        batch_loss_state = loss_fnc(input=predictions_state.squeeze(), target=label_state.long())\n",
        "        batch_loss_type = loss_fnc(input=predictions_type.squeeze(), target=label_type.long())\n",
        "\n",
        "        #Combining losses and calculating gradients\n",
        "        Overall_loss = batch_loss_state + batch_loss_type\n",
        "        Overall_loss.backward()\n",
        "        optimizer.step()\n",
        "        #evaluating Training Acc\n",
        "        _, predicted_state = torch.max(predictions_state.data, 1)\n",
        "        _,predicted_type = torch.max(predictions_type.data, 1)\n",
        "        trainAcc_state = (label_state == predicted_state).sum().item() / 64\n",
        "        trainAcc_type = (label_type == predicted_type).sum().item() /64\n",
        "\n",
        "        if i % eval_every == 0:\n",
        "            print(\"epoch: {} |||   loss type:   {}  trainAcc type:   {} |||  loss state {}  trainAcc state:  {}  \".format(e, batch_loss_type, trainAcc_type, batch_loss_state, trainAcc_state))\n",
        "\n",
        "        running_loss.append(Overall_loss.item())\n",
        "        running_accuracy.append((trainAcc_state+trainAcc_type)/2)\n",
        "        running_loss_state.append(batch_loss_state.item())\n",
        "        running_accuracy_state.append(trainAcc_state)\n",
        "        running_loss_type.append(batch_loss_type.item())\n",
        "        running_accuracy_type.append(trainAcc_type)\n",
        "        del Overall_loss\n",
        "        del batch_loss_state\n",
        "        del batch_loss_type\n",
        "        del label_state\n",
        "        del label_type\n",
        "        del predictions_state\n",
        "        del predictions_type\n",
        "\n",
        "        \n",
        "    #               LOOKING AT Validation DATA\n",
        "    for j, data_v in enumerate(valid_loader):\n",
        "            #get batch of data\n",
        "        inputs_v, label_v = data_v[0].to(device),data_v[1].to(device)\n",
        "    \n",
        "        label_state_v = torch.zeros(len(label_v)).to(device)\n",
        "        label_type_v = torch.zeros(len(label_v)).to(device)\n",
        "\n",
        "        #inputs_v, label_v = data_v\n",
        "    \n",
        "        #label_state_v = torch.zeros(len(label_v))\n",
        "        #label_type_v = torch.zeros(len(label_v))\n",
        "        for a in range(len(label_v)):\n",
        "          label_state_v[a] = relabel_state(label_v[a])\n",
        "          label_type_v[a] = relabel_type(label_v[a])\n",
        "\n",
        "          #run model on validation batch\n",
        "        predictions_state_v, predictions_type_v = model(inputs_v)\n",
        "\n",
        "            #compute loss\n",
        "        batch_valid_loss_state = loss_fnc(input=predictions_state_v.squeeze(), target=label_state_v.long())\n",
        "        batch_valid_loss_type = loss_fnc(input=predictions_type_v.squeeze(), target=label_type_v.long())\n",
        "\n",
        "        Overall_loss_v = batch_valid_loss_state + batch_valid_loss_type\n",
        "\n",
        "            #evaluate\n",
        "        _, predicted_state_v = torch.max(predictions_state_v.data, 1)\n",
        "        _,predicted_type_v = torch.max(predictions_type_v.data, 1)\n",
        "\n",
        "        validAcc_state = (label_state_v == predicted_state_v).sum().item() / 64\n",
        "        validAcc_type = (label_type_v == predicted_type_v).sum().item() / 64\n",
        "\n",
        "        if j % eval_every == 0:\n",
        "             print(\"epoch: {} |||   vloss type:   {}  validAcc type:   {} |||  vloss state {}  validAcc state:  {}  \".format(e, batch_valid_loss_type, validAcc_type, batch_valid_loss_state,validAcc_state))\n",
        "        \n",
        "        running_valid_loss.append(Overall_loss_v.item())\n",
        "        running_valid_accuracy.append((validAcc_state+validAcc_type)/2)\n",
        "    \n",
        "        running_valid_loss_state.append(batch_valid_loss_state.item())\n",
        "        running_valid_accuracy_state.append(validAcc_state)\n",
        "        running_valid_loss_type.append(batch_valid_loss_type.item())\n",
        "        running_valid_accuracy_type.append(validAcc_type)\n",
        "\n",
        "        del Overall_loss_v\n",
        "        del batch_valid_loss_state\n",
        "        del batch_valid_loss_type\n",
        "        del label_state_v\n",
        "        del label_type_v\n",
        "        del predictions_state_v\n",
        "        del predictions_type_v\n",
        "        #--------------Storing data from every epoch into lists--------------\n",
        "\n",
        "    #             lists for TRAINING accuracy\n",
        "    #Overall accuracy\n",
        "    trainacc_ = sum(running_accuracy) / float(len(running_accuracy))\n",
        "    train_acc_list.append(trainacc_)\n",
        "    #state accuracy\n",
        "    trainacc_state_ = sum(running_accuracy_state) / float(len(running_accuracy_state))\n",
        "    train_acc_list_state.append(trainacc_state_)\n",
        "    #type accuracy\n",
        "    trainAcc_type_ = sum(running_accuracy_type) / float(len(running_accuracy_type))\n",
        "    train_acc_list_type.append(trainAcc_type_)\n",
        "\n",
        "    #             lists for TRAINING losses\n",
        "    #Overall Loss\n",
        "    loss_ = sum(running_loss) / float(len(running_loss))\n",
        "    train_loss_list.append(loss_)\n",
        "    #State Loss\n",
        "    loss_state_ = sum(running_loss_state) / float(len(running_loss_state))\n",
        "    train_loss_list_state.append(loss_state_)\n",
        "    #Type Loss\n",
        "    loss_type_ = sum(running_loss_type) / float(len(running_loss_type))\n",
        "    train_loss_list_type.append(loss_type_)\n",
        "\n",
        "\n",
        "    #------------------------------------------------------------------------------------\n",
        "    #             lists for VALIDATION accuracy\n",
        "    #Overall accuracy\n",
        "    validacc_ = sum(running_valid_accuracy) / float(len(running_valid_accuracy))\n",
        "    valid_acc_list.append(validacc_)\n",
        "    #state accuracy\n",
        "    validacc_state_ = sum(running_valid_accuracy_state) / float(len(running_valid_accuracy_state))\n",
        "    valid_acc_list_state.append(validacc_state_)\n",
        "    #type accuracy\n",
        "    validAcc_type_ = sum(running_valid_accuracy_type) / float(len(running_valid_accuracy_type))\n",
        "    valid_acc_list_type.append(validAcc_type_)\n",
        "\n",
        "    #             lists for VALIDATION losses\n",
        "    #Overall Loss\n",
        "    valid_loss_ = sum(running_valid_loss) / float(len(running_valid_loss))\n",
        "    valid_loss_list.append(valid_loss_)\n",
        "    #State Loss\n",
        "    valid_loss_state_ = sum(running_valid_loss_state) / float(len(running_valid_loss_state))\n",
        "    valid_loss_list_state.append(valid_loss_state_)\n",
        "    #Type Loss\n",
        "    valid_loss_type_ = sum(running_valid_loss_type) / float(len(running_valid_loss_type))\n",
        "    valid_loss_list_type.append(valid_loss_type_)\n",
        "\n",
        "        # early_stopping needs the validation loss to check if it has decresed, \n",
        "        # and if it has, it will make a checkpoint of the current model\n",
        "    a=EarlyStopping(valid_loss_, model)\n",
        "        \n",
        "    if a.early_stop == True:\n",
        "        print(\"Early stopping\")\n",
        "        break\n",
        "\n",
        "    epoch_list.append(e)\n",
        "\n",
        "end = time()\n",
        "\n",
        "#Quick training stats\n",
        "print(\"Total training time: \", end - start)\n",
        "print(\"Epochs trained for:\",len(epoch_list))\n",
        "print(\"Max train Acc: \", max(train_acc_list))\n",
        "print(\"Max valid Acc: \", max(valid_acc_list))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3zkjQt-KbEsC"
      },
      "source": [
        "### **PLOTTING**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0tkaHoH1bLBG"
      },
      "source": [
        "if plot == True:\n",
        "        #Plot overall loss vs epoch\n",
        "        plt.plot(epoch_list, train_loss_list, label='Train')\n",
        "        plt.plot(epoch_list, valid_loss_list, label='Valid')\n",
        "        plt.title('Overall Loss vs. Epoch')\n",
        "        plt.xlabel('Epoch')\n",
        "        plt.ylabel('Loss')\n",
        "        plt.show()\n",
        "\n",
        "        #Plot overall Accuracy vs epoch\n",
        "        plt.plot(epoch_list, train_acc_list, label='Train')\n",
        "        plt.plot(epoch_list, valid_acc_list, label='Valid')\n",
        "        plt.title('Training Accuracy vs. Epoch')\n",
        "        plt.xlabel('Epoch')\n",
        "        plt.ylabel('Accuracy')\n",
        "        plt.legend()\n",
        "        plt.show()\n",
        "\n",
        "                #Plot TYPE loss vs epoch\n",
        "        plt.plot(epoch_list, train_loss_list_type, label='Train')\n",
        "        plt.plot(epoch_list, valid_loss_list_type, label='Valid')\n",
        "        plt.title('Type Loss vs. Epoch')\n",
        "        plt.xlabel('Epoch')\n",
        "        plt.ylabel('Loss')\n",
        "        plt.show()\n",
        "\n",
        "                #Plot TYPE Accuracy vs epoch\n",
        "        plt.plot(epoch_list, train_acc_list_type, label='Train')\n",
        "        plt.plot(epoch_list, valid_acc_list_type, label='Valid')\n",
        "        plt.title('Type Accuracy vs. Epoch')\n",
        "        plt.xlabel('Epoch')\n",
        "        plt.ylabel('Accuracy')\n",
        "        plt.legend()\n",
        "        plt.show()\n",
        "\n",
        "                #Plot STATE loss vs epoch\n",
        "        plt.plot(epoch_list, train_loss_list_state, label='Train')\n",
        "        plt.plot(epoch_list, valid_loss_list_state, label='Valid')\n",
        "        plt.title('State Loss vs. Epoch')\n",
        "        plt.xlabel('Epoch')\n",
        "        plt.ylabel('Loss')\n",
        "        plt.show()\n",
        "\n",
        "                #Plot STATE Accuracy vs epoch\n",
        "        plt.plot(epoch_list, train_acc_list_state, label='Train')\n",
        "        plt.plot(epoch_list, valid_acc_list_state, label='Valid')\n",
        "        plt.title('State Accuracy vs. Epoch')\n",
        "        plt.xlabel('Epoch')\n",
        "        plt.ylabel('Accuracy')\n",
        "        plt.legend()\n",
        "        plt.show()\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ff8RdZThxhnn"
      },
      "source": [
        "### **Saving model**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wEKjmzkJxsIr"
      },
      "source": [
        "if save == True:\n",
        "    torch.save(model,'/content/drive/MyDrive/ECE324 project/Figures/Multi-Head/row 20/model.pt')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XKjTO-9SdafC"
      },
      "source": [
        "### **Confusion Matrix**\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0_E-3ZjbdX7G"
      },
      "source": [
        "from sklearn.metrics import confusion_matrix, classification_report\n",
        "def confusion_classification_report(model,dataloader):\n",
        "    predlist_state=torch.zeros(0,dtype=torch.long, device='cpu')\n",
        "    lbllist_state=torch.zeros(0,dtype=torch.long, device='cpu')\n",
        "    predlist_type=torch.zeros(0,dtype=torch.long, device='cpu')\n",
        "    lbllist_type=torch.zeros(0,dtype=torch.long, device='cpu')\n",
        "    predlist=torch.zeros(0,dtype=torch.long, device='cpu')\n",
        "    lbllist=torch.zeros(0,dtype=torch.long, device='cpu')\n",
        "    \n",
        "    \n",
        "    with torch.no_grad():\n",
        "        for i, data in enumerate(dataloader):\n",
        "            inputs, label = data[0].to(device), data[1].to(device)\n",
        "            label_state = torch.zeros(len(label)).to(device)\n",
        "            label_type = torch.zeros(len(label)).to(device)\n",
        "            for k in range(len(label)):\n",
        "                label_state[k] = relabel_state(label[k])\n",
        "                label_type[k] = relabel_type(label[k])\n",
        "                \n",
        "            \n",
        "            predictions_state, predictions_type = model(inputs)\n",
        "            _, predicted_state = torch.max(predictions_state.data, 1)\n",
        "            _,predicted_type = torch.max(predictions_type.data, 1)\n",
        "    \n",
        "            # Append batch prediction results\n",
        "            predlist_state=torch.cat([predlist_state,predicted_state.view(-1).cpu()])\n",
        "            lbllist_state=torch.cat([lbllist_state,label_state.view(-1).cpu()])\n",
        "            predlist_type=torch.cat([predlist_type,predicted_type.view(-1).cpu()])\n",
        "            lbllist_type=torch.cat([lbllist_type,label_type.view(-1).cpu()])\n",
        "            \n",
        "            predicted = torch.zeros(len(label)).to(device)\n",
        "            # 15 label confusion matrix\n",
        "            for k in range(len(predicted_state)):\n",
        "                predicted[k] = recombine_prediction(predicted_state[k],predicted_type[k])\n",
        "            predlist=torch.cat([predlist,predicted.view(-1).cpu()])\n",
        "            lbllist=torch.cat([lbllist,label.view(-1).cpu()])\n",
        "     \n",
        "    # Confusion matrix and classification report\n",
        "    conf_mat_state=confusion_matrix(lbllist_state.numpy(), predlist_state.numpy())\n",
        "    conf_mat_type=confusion_matrix(lbllist_type.numpy(), predlist_type.numpy())\n",
        "    conf_mat=confusion_matrix(lbllist.numpy(), predlist.numpy())\n",
        "    class_report_state = classification_report(lbllist_state.numpy(), predlist_state.numpy())\n",
        "    class_report_type = classification_report(lbllist_type.numpy(), predlist_type.numpy())\n",
        "    class_report = classification_report(lbllist.numpy(), predlist.numpy())\n",
        "    \n",
        "    # print(conf_mat)\n",
        "    # print(conf_mat_state)\n",
        "    # print(conf_mat_type)\n",
        "    print(class_report_state)\n",
        "    print(class_report_type)\n",
        "    print(class_report)\n",
        "    \n",
        "    # Per-class accuracy\n",
        "    class_accuracy=100*conf_mat.diagonal()/conf_mat.sum(1)\n",
        "    print(class_accuracy)\n",
        "    class_accuracy_state=100*conf_mat_state.diagonal()/conf_mat_state.sum(1)\n",
        "    print(class_accuracy_state)\n",
        "    class_accuracy_type=100*conf_mat_type.diagonal()/conf_mat_type.sum(1)\n",
        "    print(class_accuracy_type)\n",
        "    return conf_mat_state, conf_mat_type, conf_mat, class_report_state, class_report_type, class_report"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bvdrwwYBxEZH"
      },
      "source": [
        "model_loaded = torch.load(r\"/content/drive/MyDrive/ECE324 project/Figures/Multi-Head/row 20/model.pt\")\n",
        "if confusion:\n",
        "    conf_mat_state, conf_mat_type, conf_mat, class_report_state, class_report_type, class_report = confusion_classification_report(model_loaded, valid_loader)\n",
        "\n",
        "    plt.figure()\n",
        "    total_map = sns.heatmap(conf_mat,cmap=\"YlGnBu\", annot=True,fmt='g',square = True,xticklabels=['ri_A', 'ri_B', 'ri_O', 'ri_R', 'ri_S', 'ro_A', 'ro_B', 'ro_O', 'ro_R', 'ro_S', 'ur_A', 'ur_B', 'ur_O', 'ur_R', 'ur_S'], yticklabels=['ri_A', 'ri_B', 'ri_O', 'ri_R', 'ri_S', 'ro_A', 'ro_B', 'ro_O', 'ro_R', 'ro_S', 'ur_A', 'ur_B', 'ur_O', 'ur_R', 'ur_S'] )\n",
        "    bottom, top = total_map.get_ylim()\n",
        "    total_map.set_ylim(bottom + 0.5, top - 0.5)\n",
        "    total_map.set_title('Total Confusion Matrix')\n",
        "    total_map.set_ylabel('True Label')\n",
        "    total_map.set_xlabel('Predicted Label')\n",
        "    plt.show()\n",
        "\n",
        "    plt.figure()\n",
        "    state_map = sns.heatmap(conf_mat_state,cmap=\"YlGnBu\", annot=True,fmt='g',square = True,xticklabels=['Ripe', 'Rotten', 'Unripe'], yticklabels=['Ripe', 'Rotten', 'Unripe'] )\n",
        "    bottom, top = state_map.get_ylim()\n",
        "    state_map.set_ylim(bottom + 0.5, top - 0.5)\n",
        "    state_map.set_title('State Confusion Matrix')\n",
        "    state_map.set_ylabel('True Label')\n",
        "    state_map.set_xlabel('Predicted Label')\n",
        "    plt.show()\n",
        "\n",
        "    plt.figure()\n",
        "    type_map = sns.heatmap(conf_mat_type,cmap=\"YlGnBu\", annot=True,fmt='g',square = True,xticklabels=['Apple','Banna','Orange','Raspberry','Strawberry'], yticklabels=['Apple','Banna','Orange','Raspberry','Strawberry'] )\n",
        "    bottom, top = type_map.get_ylim()\n",
        "    type_map.set_ylim(bottom + 0.5, top - 0.5)\n",
        "    type_map.set_title('Type Confusion Matrix')\n",
        "    type_map.set_ylabel('True Label')\n",
        "    type_map.set_xlabel('Predicted Label')\n",
        "    plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "COIGpQ80teMX"
      },
      "source": [
        "# **TEST SETS**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FQw07sFMmbtm"
      },
      "source": [
        "test_white =  torchvision.datasets.ImageFolder(root='/content/drive/MyDrive/ECE324 project/test_white_bg', transform=my_transform)\n",
        "test_varied =  torchvision.datasets.ImageFolder(root='/content/drive/MyDrive/ECE324 project/test_varied_bg', transform=my_transform)\n",
        "test_green =  torchvision.datasets.ImageFolder(root='/content/drive/MyDrive/ECE324 project/test_green_bg', transform=my_transform)\n",
        "\n",
        "test_white_loader = DataLoader(test_white, batch_size=4)\n",
        "test_varied_loader = DataLoader(test_varied, batch_size=4)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BXLvg_jnvK_O"
      },
      "source": [
        "### **White background test set**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tQfOX0CsmPqm"
      },
      "source": [
        "running_loss = []\n",
        "running_accuracy = []\n",
        "running_loss_state = []\n",
        "running_accuracy_state = []\n",
        "running_loss_type = []\n",
        "running_accuracy_type = []\n",
        "for i, data in enumerate(test_white_loader):\n",
        "        #get batch of data\n",
        "    inputs, label = data[0].to(device), data[1].to(device)\n",
        "\n",
        "    label_state = torch.zeros(len(label)).to(device)\n",
        "    label_type = torch.zeros(len(label)).to(device)\n",
        "    for k in range(len(label)):\n",
        "        label_state[k] = relabel_state(label[k])\n",
        "        label_type[k] = relabel_type(label[k]) \n",
        "\n",
        "    #run model on validation batch\n",
        "    predictions_state, predictions_type = model_loaded(inputs)\n",
        "\n",
        "        #compute loss\n",
        "    batch_loss_state = loss_fnc(input=predictions_state.squeeze(), target=label_state.long())\n",
        "    batch_loss_type = loss_fnc(input=predictions_type.squeeze(), target=label_type.long())\n",
        "\n",
        "    Overall_loss = batch_loss_state + batch_loss_type\n",
        "        #evaluate\n",
        "    _, predicted_state = torch.max(predictions_state.data, 1)\n",
        "    _,predicted_type = torch.max(predictions_type.data, 1)\n",
        "\n",
        "    Acc_state = (label_state == predicted_state).sum().item() / 4\n",
        "    Acc_type = (label_type == predicted_type).sum().item() / 4\n",
        "    running_loss.append(Overall_loss.item())\n",
        "    running_accuracy.append((Acc_state+Acc_type)/2)\n",
        "\n",
        "    running_loss_state.append(batch_loss_state.item())\n",
        "    running_accuracy_state.append(Acc_state)\n",
        "    running_loss_type.append(batch_loss_type.item())\n",
        "    running_accuracy_type.append(Acc_type) \n",
        "    \n",
        "#Overall accuracy\n",
        "whiteacc_ = sum(running_accuracy) / float(len(running_accuracy))\n",
        "#state accuracy\n",
        "whiteacc_state_ = sum(running_accuracy_state) / float(len(running_accuracy_state))\n",
        "\n",
        "#type accuracy\n",
        "whiteAcc_type_ = sum(running_accuracy_type) / float(len(running_accuracy_type))\n",
        "\n",
        "#             lists for VALIDATION losses\n",
        "#Overall Loss\n",
        "white_loss_ = sum(running_loss) / float(len(running_loss))\n",
        "\n",
        "#State Loss\n",
        "white_loss_state_ = sum(running_loss_state) / float(len(running_loss_state))\n",
        "\n",
        "#Type Loss\n",
        "white_loss_type_ = sum(running_loss_type) / float(len(running_loss_type))\n",
        "\n",
        "print(whiteacc_)\n",
        "print(white_loss_)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "urt5dO7SvR0r"
      },
      "source": [
        "### **Varied background test dataset**\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wcqhZ8EszGD4"
      },
      "source": [
        "running_loss = []\n",
        "running_accuracy = []\n",
        "running_loss_state = []\n",
        "running_accuracy_state = []\n",
        "running_loss_type = []\n",
        "running_accuracy_type = []\n",
        "for i, data in enumerate(test_varied_loader):\n",
        "        #get batch of data\n",
        "    inputs, label = data[0].to(device), data[1].to(device)\n",
        "\n",
        "    label_state = torch.zeros(len(label)).to(device)\n",
        "    label_type = torch.zeros(len(label)).to(device)\n",
        "    for k in range(len(label)):\n",
        "        label_state[k] = relabel_state(label[k])\n",
        "        label_type[k] = relabel_type(label[k]) \n",
        "\n",
        "    #run model on validation batch\n",
        "    predictions_state, predictions_type = model_loaded(inputs)\n",
        "\n",
        "        #compute loss\n",
        "    batch_loss_state = loss_fnc(input=predictions_state.squeeze(), target=label_state.long())\n",
        "    batch_loss_type = loss_fnc(input=predictions_type.squeeze(), target=label_type.long())\n",
        "\n",
        "    Overall_loss = batch_loss_state + batch_loss_type\n",
        "        #evaluate\n",
        "    _, predicted_state = torch.max(predictions_state.data, 1)\n",
        "    _,predicted_type = torch.max(predictions_type.data, 1)\n",
        "\n",
        "    Acc_state = (label_state == predicted_state).sum().item() / 4\n",
        "    Acc_type = (label_type == predicted_type).sum().item() / 4\n",
        "    running_loss.append(Overall_loss.item())\n",
        "    running_accuracy.append((Acc_state+Acc_type)/2)\n",
        "\n",
        "    running_loss_state.append(batch_loss_state.item())\n",
        "    running_accuracy_state.append(Acc_state)\n",
        "    running_loss_type.append(batch_loss_type.item())\n",
        "    running_accuracy_type.append(Acc_type) \n",
        "    \n",
        "#Overall accuracy\n",
        "variedacc_ = sum(running_accuracy) / float(len(running_accuracy))\n",
        "#state accuracy\n",
        "variedacc_state_ = sum(running_accuracy_state) / float(len(running_accuracy_state))\n",
        "\n",
        "#type accuracy\n",
        "variedAcc_type_ = sum(running_accuracy_type) / float(len(running_accuracy_type))\n",
        "\n",
        "#             lists for VALIDATION losses\n",
        "#Overall Loss\n",
        "varied_loss_ = sum(running_loss) / float(len(running_loss))\n",
        "\n",
        "#State Loss\n",
        "varied_loss_state_ = sum(running_loss_state) / float(len(running_loss_state))\n",
        "\n",
        "#Type Loss\n",
        "varied_loss_type_ = sum(running_loss_type) / float(len(running_loss_type))\n",
        "\n",
        "print(variedacc_)\n",
        "print(varied_loss_)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pbye37yKvW_y"
      },
      "source": [
        "### **General Test set**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SR_whZTXzIoU"
      },
      "source": [
        "running_loss = []\n",
        "running_accuracy = []\n",
        "running_loss_state = []\n",
        "running_accuracy_state = []\n",
        "running_loss_type = []\n",
        "running_accuracy_type = []\n",
        "for i, data in enumerate(test_loader):\n",
        "        #get batch of data\n",
        "    inputs, label = data[0].to(device), data[1].to(device)\n",
        "\n",
        "    label_state = torch.zeros(len(label)).to(device)\n",
        "    label_type = torch.zeros(len(label)).to(device)\n",
        "    for k in range(len(label)):\n",
        "        label_state[k] = relabel_state(label[k])\n",
        "        label_type[k] = relabel_type(label[k]) \n",
        "\n",
        "    #run model on validation batch\n",
        "    predictions_state, predictions_type = model_loaded(inputs)\n",
        "\n",
        "        #compute loss\n",
        "    batch_loss_state = loss_fnc(input=predictions_state.squeeze(), target=label_state.long())\n",
        "    batch_loss_type = loss_fnc(input=predictions_type.squeeze(), target=label_type.long())\n",
        "\n",
        "    Overall_loss = batch_loss_state + batch_loss_type\n",
        "        #evaluate\n",
        "    _, predicted_state = torch.max(predictions_state.data, 1)\n",
        "    _,predicted_type = torch.max(predictions_type.data, 1)\n",
        "\n",
        "    Acc_state = (label_state == predicted_state).sum().item() / 64\n",
        "    Acc_type = (label_type == predicted_type).sum().item() / 64\n",
        "    running_loss.append(Overall_loss.item())\n",
        "    running_accuracy.append((Acc_state+Acc_type)/2)\n",
        "\n",
        "    running_loss_state.append(batch_loss_state.item())\n",
        "    running_accuracy_state.append(Acc_state)\n",
        "    running_loss_type.append(batch_loss_type.item())\n",
        "    running_accuracy_type.append(Acc_type) \n",
        "    \n",
        "#Overall accuracy\n",
        "testacc_ = sum(running_accuracy) / float(len(running_accuracy))\n",
        "#state accuracy\n",
        "testacc_state_ = sum(running_accuracy_state) / float(len(running_accuracy_state))\n",
        "\n",
        "#type accuracy\n",
        "testAcc_type_ = sum(running_accuracy_type) / float(len(running_accuracy_type))\n",
        "\n",
        "#             lists for VALIDATION losses\n",
        "#Overall Loss\n",
        "test_loss_ = sum(running_loss) / float(len(running_loss))\n",
        "\n",
        "#State Loss\n",
        "test_loss_state_ = sum(running_loss_state) / float(len(running_loss_state))\n",
        "\n",
        "#Type Loss\n",
        "test_loss_type_ = sum(running_loss_type) / float(len(running_loss_type))\n",
        "\n",
        "print(testacc_)\n",
        "print(test_loss_)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}