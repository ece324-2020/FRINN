{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Transfer_Learning_Conv_Layer.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"code","metadata":{"id":"kvMLiZpxDWXE"},"source":["import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","import numpy as np\n","import torchvision\n","from torchvision import datasets, models, transforms\n","import matplotlib.pyplot as plt\n","import os                                                                                      \n","import matplotlib.pyplot as plt                                 \n","from sklearn.preprocessing import LabelEncoder, OneHotEncoder               \n","import torchvision.transforms as transforms              \n","from torch.utils.data import SubsetRandomSampler \n","from torch.utils.data import Dataset, DataLoader\n","import torch.utils.data as data   \n","from torchsummary import summary\n","import torch.nn.functional as F\n","from time import time\n","from sklearn.metrics import confusion_matrix, classification_report\n","from torchvision import models\n","import seaborn as sns"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"JSaqmCmUJknC","colab":{"base_uri":"https://localhost:8080/"},"outputId":"061f7fb0-509b-4084-faac-abf919001f0b"},"source":["from google.colab import drive\n","\n","drive.mount('/content/drive', force_remount=True)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Mounted at /content/drive\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"ZlPHBpiTreCI"},"source":["torch.cuda.empty_cache()\n","device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"y_Qe76DLrNci"},"source":["# VGG with Batch Norm with Different kinds of Freezing"]},{"cell_type":"code","metadata":{"id":"U54vzEmqED29"},"source":["vgg = models.vgg16_bn(pretrained=True)\n","for param in vgg.features.parameters():\n","    param.requires_grad = False"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"3ARVDIyoGaKK"},"source":["# Unfreeze upto 34\n","# ct = 0\n","# for child in vgg.children():\n","#     ct += 1\n","#     if ct < 34:\n","#         for param in child.parameters():\n","#             param.requires_grad = False"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"NZ2zFlBHKmXF"},"source":["#Convolutional Layer with Batch Norm\n","vgg.features[43] = nn.Sequential(vgg.features[43],nn.Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)), nn.BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True),\n","                nn.ReLU(inplace=True),nn.MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False))\n","#                 \n","#Two Convolutional Layer with Batch Norm\n","# vgg.features[43] = nn.Sequential(vgg.features[43],nn.Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)), nn.BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True),\n","#                 nn.ReLU(inplace=True),nn.MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False),nn.Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)), nn.BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True),\n","#                 nn.ReLU(inplace=True),nn.MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False))\n","                \n","print(nn.Sequential(*list(vgg.children())[:-1]))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"CCo4OIkergJ2"},"source":["# Data Loading"]},{"cell_type":"code","metadata":{"id":"AgUhZjKmb_O5"},"source":["#Loading Data \n","my_transform = transforms.Compose([\n","  transforms.ToTensor(),\n","  transforms.ToPILImage(),\n","  transforms.Resize((224,224)),\n","  transforms.ToTensor(),\n","  transforms.Normalize(mean = [0.66445047,0.55465436,0.447036], std = [0.321551,0.33547384,0.3524585])\n","  ])\n","dataset =  torchvision.datasets.ImageFolder(r\"C:\\Users\\Mark\\Desktop\\3rd year 1st term\\ECE324\\Project\\Clean Data\\Clean Data\", transform=my_transform)\n","\n","train_data, valid_data, test_data = torch.utils.data.random_split(dataset, [9600,2400,3000], generator=torch.Generator().manual_seed(0))\n","\n","train_loader = DataLoader(train_data, batch_size=64,shuffle= True)                  \n","valid_loader = DataLoader(valid_data, batch_size=64,shuffle= False)\n","test_loader = DataLoader(test_data, batch_size=64,shuffle= False)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Hi5Uf0HPrjpl"},"source":["# Relabel Functions"]},{"cell_type":"code","metadata":{"id":"ItAyjdHpY6Nw"},"source":["# Relabel Functions\n","def relabel_state(argument):\n","    switcher = {\n","        0: 0,\n","        1: 0,\n","        2: 0,\n","        3: 0,\n","        4: 0,\n","        5: 1,\n","        6: 1,\n","        7: 1,\n","        8: 1,\n","        9: 1,\n","        10:2,\n","        11:2,\n","        12:2,\n","        13:2,\n","        14:2}\n","    return switcher[argument.item()]\n","\n","def relabel_type(argument):\n","    switcher = {\n","        0: 0,\n","        1: 1,\n","        2: 2,\n","        3: 3,\n","        4: 4,\n","        5: 0,\n","        6: 1,\n","        7: 2,\n","        8: 3,\n","        9: 4,\n","        10:0,\n","        11:1,\n","        12:2,\n","        13:3,\n","        14:4}\n","    return switcher[argument.item()]\n","\n","def recombine_prediction(argument1,argument2):\n","    switcher = {\n","         str([0,0]): 0,\n","         str([0,1]): 1,\n","         str([0,2]): 2,\n","         str([0,3]): 3,\n","         str([0,4]): 4,\n","         str([1,0]): 5,\n","         str([1,1]): 6,\n","         str([1,2]): 7,\n","         str([1,3]): 8,\n","         str([1,4]): 9,\n","         str([2,0]):10,\n","         str([2,1]):11,\n","         str([2,2]):12,\n","         str([2,3]):13,\n","         str([2,4]):14}\n","    return switcher[str([argument1.item(),argument2.item()])]"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"PfH6wBMYrm8b"},"source":["# Early Stopping"]},{"cell_type":"code","metadata":{"id":"nV6sti9dqaLR"},"source":["class EarlyStopping:\n","    \"\"\"Early stops the training if validation loss doesn't improve after a given patience.\"\"\"\n","    def __init__(self, patience=3, verbose=True, delta=0.001, trace_func=print):\n","        \"\"\"\n","        Args:\n","            patience (int): How long to wait after last time validation loss improved.\n","                            Default: 3\n","            verbose (bool): If True, prints a message for each validation loss improvement. \n","                            Default: False\n","            delta (float): Minimum change in the monitored quantity to qualify as an improvement.\n","                            Default: 0\n","            trace_func (function): trace print function.\n","                            Default: print            \n","        \"\"\"\n","        self.patience = patience\n","        self.verbose = verbose\n","        self.counter = 0\n","        self.best_score = None\n","        self.early_stop = False\n","        self.val_loss_min = np.Inf\n","        self.delta = delta\n","        self.trace_func = trace_func\n","    def __call__(self, val_loss):\n","\n","        score = -val_loss\n","\n","        if self.best_score is None:\n","            self.best_score = score\n","            self.save_checkpoint(val_loss)\n","        elif score < self.best_score + self.delta:\n","            self.counter += 1\n","            self.trace_func(f'EarlyStopping counter: {self.counter} out of {self.patience}')\n","            if self.counter >= self.patience:\n","                self.early_stop = True\n","        else:\n","            self.best_score = score\n","            self.save_checkpoint(val_loss)\n","            self.counter = 0\n","\n","    def save_checkpoint(self, val_loss):\n","        '''Saves model when validation loss decrease.'''\n","        if self.verbose:\n","            self.trace_func(f'Validation loss decreased ({self.val_loss_min:.6f} --> {val_loss:.6f}).')\n","        self.val_loss_min = val_loss"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"KBS77IFtrs0l"},"source":["# Various Models"]},{"cell_type":"code","metadata":{"id":"drwt0GEkEHtg"},"source":["#Transfer Learning Multi Head\n","class MyVgg(nn.Module):\n","    def __init__(self,originalmodel):\n","        super(MyVgg,self).__init__()\n","        vgg = originalmodel\n","        # Here you get the bottleneck/feature extractor\n","        self.vgg_feature_extractor = nn.Sequential(*list(vgg.children())[:-1])\n","        self.classifier1 = nn.Sequential(\n","            nn.Linear(512 * 7 * 7, 4096),\n","            nn.ReLU(True),\n","            nn.Linear(4096, 4096),\n","            nn.ReLU(True),\n","            nn.Linear(4096, 3),\n","            nn.Sigmoid()\n","        )\n","        self.classifier2 = nn.Sequential(\n","            nn.Linear(512 * 7 * 7, 4096),\n","            nn.ReLU(True),\n","            nn.Linear(4096, 4096),\n","            nn.ReLU(True),\n","            nn.Linear(4096, 5),\n","            nn.Sigmoid()\n","                            )\n","\n","    # Set your own forward pass\n","    def forward(self, img, extra_info=None):\n","        \n","        x = self.vgg_feature_extractor(img)\n","        x = x.view(x.size(0), -1)\n","        x1 = self.classifier1(x)\n","        x2 = self.classifier2(x)\n","\n","        return x1, x2\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"7EMgYjj3GpK6"},"source":["# Extra MLP Layer for State\n","# class MyVggextra(nn.Module):\n","#     def __init__(self,originalmodel):\n","#         super(MyVggextra,self).__init__()\n","#         vgg = originalmodel\n","#         # Here you get the bottleneck/feature extractor\n","#         self.vgg_feature_extractor = nn.Sequential(*list(vgg.children())[:-1])\n","#         self.classifier1 = nn.Sequential(\n","#             nn.Linear(512 * 7 * 7, 4096),\n","#             nn.ReLU(True),\n","#             nn.Linear(4096, 4096),\n","#             nn.ReLU(True),\n","#             nn.Linear(4096, 4096),\n","#             nn.ReLU(True),\n","#             nn.Linear(4096, 4096),\n","#             nn.ReLU(True),\n","#             nn.Linear(4096, 4096),\n","#             nn.ReLU(True),\n","#             nn.Linear(4096, 3),\n","#             nn.Sigmoid()\n","#         )\n","#         self.classifier2 = nn.Sequential(\n","#             nn.Linear(512 * 7 * 7, 4096),\n","#             nn.ReLU(True),\n","#             nn.Linear(4096, 4096),\n","#             nn.ReLU(True),\n","#             nn.Linear(4096, 5),\n","#             nn.Sigmoid()\n","#                             )\n","# \n","#     # Set your own forward pass\n","#     def forward(self, img, extra_info=None):\n","#         \n","#         x = self.vgg_feature_extractor(img)\n","#         x = x.view(x.size(0), -1)\n","#         x1 = self.classifier1(x)\n","#         x2 = self.classifier2(x)\n","# \n","#         return x1, x2"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"T_Vs1SReGvMF"},"source":["# Convolutional Layer on each head\n","# class MyVggeach(nn.Module):\n","#     def __init__(self,originalmodel):\n","#         super(MyVggeach,self).__init__()\n","#         vgg = originalmodel\n","#         # Here you get the bottleneck/feature extractor\n","#         self.vgg_feature_extractor = nn.Sequential(*list(vgg.children())[:-1])\n","#         self.convlayer1 = nn.Sequential(nn.Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)), nn.BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True),\n","#                 nn.ReLU(inplace=True),nn.MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False))\n","#         self.convlayer2 = nn.Sequential(nn.Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)), nn.BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True),\n","#                 nn.ReLU(inplace=True),nn.MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False),nn.Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)), nn.BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True),\n","#                 nn.ReLU(inplace=True),nn.MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False))\n","#         self.classifier1 = nn.Sequential(\n","#             nn.Linear(512 * 7 * 7, 4096),\n","#             nn.ReLU(True),\n","#             nn.Linear(4096, 4096),\n","#             nn.ReLU(True),\n","#             nn.Linear(4096, 4096),\n","#             nn.ReLU(True),\n","#             nn.Linear(4096, 4096),\n","#             nn.ReLU(True),\n","#             nn.Linear(4096, 4096),\n","#             nn.ReLU(True),\n","#             nn.Linear(4096, 3),\n","#             nn.Sigmoid()\n","#         )\n","#         self.classifier2 = nn.Sequential(\n","#             nn.Linear(512 * 7 * 7, 4096),\n","#             nn.ReLU(True),\n","#             nn.Linear(4096, 4096),\n","#             nn.ReLU(True),\n","#             nn.Linear(4096, 5),\n","#             nn.Sigmoid()\n","#                             )\n","# \n","#     # Set your own forward pass\n","#     def forward(self, img, extra_info=None):\n","#         \n","#         x = self.vgg_feature_extractor(img)\n","#         x1= self.convlayer1(x)\n","#         x2= self.convlayer2(x)\n","#         x1 = x.view(x1.size(0), -1)\n","#         x2 = x.view(x2.size(0), -1)\n","#         #torch.Size([64, 25088])\n","#         x1 = self.classifier1(x1)\n","#         x2 = self.classifier2(x2)\n","# \n","#         return x1, x2"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"4eTt8WGQrzEs"},"source":["Initialize Model and various functions"]},{"cell_type":"code","metadata":{"id":"gyBWXzBqZxwC"},"source":["#MODEL ARGUMENTS\n","lr = 0.1\n","epochs = 20\n","eval_every = 60\n","model = MyVgg(vgg)\n","model = model.to(device)\n","\n","optimizer = torch.optim.SGD(model.parameters(), lr=lr)\n","scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', patience=3,verbose=True)\n","\n","loss_fnc= torch.nn.CrossEntropyLoss()\n","loss_fnc =loss_fnc.to(device)\n","\n","\n","#OUTPUT DATA ARGUMENTS\n","seed = 1\n","torch.manual_seed(seed)\n","save= True\n","confusion=True\n","plot = True"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"JTVijfkysC37"},"source":["Training Loop"]},{"cell_type":"code","metadata":{"id":"KYYqMqYDaANv"},"source":["train_loss_list = []\n","train_acc_list = []\n","train_loss_list_state = []\n","train_acc_list_state = []\n","train_loss_list_type = []\n","train_acc_list_type = []\n","\n","valid_loss_list = []\n","valid_acc_list = []\n","valid_loss_list_state = []\n","valid_acc_list_state = []\n","valid_loss_list_type = []\n","valid_acc_list_type = []\n","\n","epoch_num = []         #Epoch number\n","start = time()\n","for e in range(epochs):\n","    print(\"Time:    \", time() - start)\n","    running_loss = []\n","    running_accuracy = []\n","    running_loss_state = []\n","    running_accuracy_state = []\n","    running_loss_type = []\n","    running_accuracy_type = []\n","\n","    running_valid_loss = []\n","    running_valid_accuracy = []\n","    running_valid_loss_state = []\n","    running_valid_accuracy_state = []\n","    running_valid_loss_type = []\n","    running_valid_accuracy_type = []\n","    model.train()\n","  #               LOOKING AT TRAINING DATA\n","\n","    for i, data in enumerate(train_loader):\n","\n","            #get batch of data\n","        inputs, label = data[0].to(device), data[1].to(device)\n","    \n","        label_state = torch.zeros(len(label)).to(device)\n","        label_type = torch.zeros(len(label)).to(device)\n","        for k in range(len(label)):\n","          label_state[k] = relabel_state(label[k])\n","          label_type[k] = relabel_type(label[k])\n","        \n","        # Setting gradients to zero and running model on batch\n","        optimizer.zero_grad()\n","        predictions_state, predictions_type = model(inputs)\n","\n","        # Computing loss based on Cross Entropy\n","        batch_loss_state = loss_fnc(input=predictions_state.squeeze(), target=label_state.long())\n","        batch_loss_type = loss_fnc(input=predictions_type.squeeze(), target=label_type.long())\n","\n","        # Combining losses and calculating gradients\n","        Overall_loss = batch_loss_state + batch_loss_type\n","        Overall_loss.backward()\n","        optimizer.step()\n","        # Evaluating Training Acc\n","        _, predicted_state = torch.max(predictions_state.data, 1)\n","        _,predicted_type = torch.max(predictions_type.data, 1)\n","        trainAcc_state = (label_state == predicted_state).sum().item() / 64\n","        trainAcc_type = (label_type == predicted_type).sum().item() / 64\n","\n","        if i % eval_every == 0:\n","           print(\"epoch: {} {} |||   loss type:   {}  trainAcc type:   {} |||  loss state {}  trainAcc state:  {}  \".format(e+1,i, batch_loss_type, trainAcc_type, batch_loss_state, trainAcc_state))\n","\n","        running_loss.append(Overall_loss.item())\n","        running_accuracy.append((trainAcc_state+trainAcc_type)/2)\n","        running_loss_state.append(batch_loss_state.item())\n","        running_accuracy_state.append(trainAcc_state)\n","        running_loss_type.append(batch_loss_type.item())\n","        running_accuracy_type.append(trainAcc_type)\n","\n","        del Overall_loss\n","        del batch_loss_state\n","        del batch_loss_type\n","        del label_state\n","        del label_type\n","        del predictions_state\n","        del predictions_type\n","\n","    \n","    # \n","    model.eval()\n","    for j, data in enumerate(valid_loader):\n","            #get batch of data\n","        inputs, label = data[0].to(device), data[1].to(device)\n","    \n","        vlabel_state = torch.zeros(len(label)).to(device)\n","        vlabel_type = torch.zeros(len(label)).to(device)\n","        for k in range(len(label)):\n","            vlabel_state[k] = relabel_state(label[k])\n","            vlabel_type[k] = relabel_type(label[k]) \n","    \n","        #run model on validation batch\n","        predictions_state_v, predictions_type_v = model(inputs)\n","    \n","            #compute loss\n","        batch_valid_loss_state = loss_fnc(input=predictions_state_v.squeeze(), target=vlabel_state.long())\n","        batch_valid_loss_type = loss_fnc(input=predictions_type_v.squeeze(), target=vlabel_type.long())\n","    \n","        Overall_loss_v = batch_valid_loss_state + batch_valid_loss_type\n","            #evaluate\n","        _, predicted_state_v = torch.max(predictions_state_v.data, 1)\n","        _,predicted_type_v = torch.max(predictions_type_v.data, 1)\n","    \n","        validAcc_state = (vlabel_state == predicted_state_v).sum().item() / 64\n","        validAcc_type = (vlabel_type == predicted_type_v).sum().item() / 64\n","        \n","        if j % eval_every == 0:\n","            print(\"epoch: {} |||   vloss type:   {}  validAcc type:   {} |||  vloss state {}  validAcc state:  {}  \".format(e+1, batch_valid_loss_type, validAcc_type, batch_valid_loss_state,validAcc_state))\n","        \n","        running_valid_loss.append(Overall_loss_v.item())\n","        running_valid_accuracy.append((validAcc_state+validAcc_type)/2)\n","    \n","        running_valid_loss_state.append(batch_valid_loss_state.item())\n","        running_valid_accuracy_state.append(validAcc_state)\n","        running_valid_loss_type.append(batch_valid_loss_type.item())\n","        running_valid_accuracy_type.append(validAcc_type) \n","        del Overall_loss_v\n","        del batch_valid_loss_state\n","        del batch_valid_loss_type\n","        del vlabel_state\n","        del vlabel_type\n","        del predicted_state_v\n","        del predicted_type_v\n","    #     \n","    #Overall accuracy\n","    trainacc_ = sum(running_accuracy) / float(len(running_accuracy))\n","    train_acc_list.append(trainacc_)\n","    #state accuracy\n","    trainacc_state_ = sum(running_accuracy_state) / float(len(running_accuracy_state))\n","    train_acc_list_state.append(trainacc_state_)\n","    #type accuracy\n","    trainAcc_type_ = sum(running_accuracy_type) / float(len(running_accuracy_type))\n","    train_acc_list_type.append(trainAcc_type_) \n","#     \n","    #             lists for TRAINING losses\n","    #Overall Loss\n","    loss_ = sum(running_loss) / float(len(running_loss))\n","    train_loss_list.append(loss_)\n","    #State Loss\n","    loss_state_ = sum(running_loss_state) / float(len(running_loss_state))\n","    train_loss_list_state.append(loss_state_)\n","    #Type Loss\n","    loss_type_ = sum(running_loss_type) / float(len(running_loss_type))\n","    train_loss_list_type.append(loss_type_) \n","        \n","    #------------------------------------------------------------------------------------\n","    #             lists for VALIDATION accuracy\n","    #Overall accuracy\n","    validacc_ = sum(running_valid_accuracy) / float(len(running_valid_accuracy))\n","    valid_acc_list.append(validacc_)\n","    #state accuracy\n","    validacc_state_ = sum(running_valid_accuracy_state) / float(len(running_valid_accuracy_state))\n","    valid_acc_list_state.append(validacc_state_)\n","    #type accuracy\n","    validAcc_type_ = sum(running_valid_accuracy_type) / float(len(running_valid_accuracy_type))\n","    valid_acc_list_type.append(validAcc_type_)\n","    \n","    #             lists for VALIDATION losses\n","    #Overall Loss\n","    valid_loss_ = sum(running_valid_loss) / float(len(running_valid_loss))\n","    valid_loss_list.append(valid_loss_)\n","    #State Loss\n","    valid_loss_state_ = sum(running_valid_loss_state) / float(len(running_valid_loss_state))\n","    valid_loss_list_state.append(valid_loss_state_)\n","    #Type Loss\n","    valid_loss_type_ = sum(running_valid_loss_type) / float(len(running_valid_loss_type))\n","    valid_loss_list_type.append(valid_loss_type_)\n","    \n","    earlystop = EarlyStopping(valid_loss_)\n","    print(earlystop.counter)\n","    if earlystop.early_stop:\n","        print(\"Early stopping\")\n","        break\n","    scheduler.step(valid_loss_)\n","    \n","    epoch_num.append(e)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"xanin3nQqpJt"},"source":["if save:\n","    torch.save(model, r\"C:\\Users\\Mark\\Desktop\\3rd year 1st term\\ECE324\\Project\\Transfer Learning\\Model\\20epoch.pt\")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"9fqKGVk5EJi2"},"source":["#Plot overall loss vs epoch\n","if plot:\n","    plt.figure()\n","    plt.plot(epoch_num, train_loss_list, label='Train')\n","    plt.plot(epoch_num, valid_loss_list, label='Valid')\n","    plt.title('Overall Loss vs. Epoch')\n","    plt.xlabel('Epoch')\n","    plt.ylabel('Loss')\n","    plt.show()\n","    plt.savefig(r\"C:\\Users\\Mark\\Desktop\\3rd year 1st term\\ECE324\\Project\\Transfer Learning\\Plot\\Overall Loss vs. Epoch.png\")\n","\n","    #Plot overall Accuracy vs epoch\n","    plt.figure()\n","    plt.plot(epoch_num, train_acc_list, label='Train')\n","    plt.plot(epoch_num, valid_acc_list, label='Valid')\n","    plt.title('Overall Accuracy vs. Epoch')\n","    plt.xlabel('Epoch')\n","    plt.ylabel('Accuracy')\n","    plt.legend()\n","    plt.show()\n","    plt.savefig(r\"C:\\Users\\Mark\\Desktop\\3rd year 1st term\\ECE324\\Project\\Transfer Learning\\Plot\\Overall Accuracy vs. Epoch.png\")\n","\n","    print('train ',max(train_acc_list), 'valid ', max(valid_acc_list))\n","\n","    #Plot TYPE loss vs epoch\n","    plt.figure()    \n","    plt.plot(epoch_num, train_loss_list_type, label='Train')\n","    plt.plot(epoch_num, valid_loss_list_type, label='Valid')\n","    plt.title('Type Loss vs. Epoch')\n","    plt.xlabel('Epoch')\n","    plt.ylabel('Loss')\n","    plt.show()\n","    plt.savefig(r\"C:\\Users\\Mark\\Desktop\\3rd year 1st term\\ECE324\\Project\\Transfer Learning\\Plot\\Type Loss vs. Epoch.png\")\n","\n","    #Plot TYPE Accuracy vs epoch\n","    plt.figure()\n","    plt.plot(epoch_num, train_acc_list_type, label='Train')\n","    plt.plot(epoch_num, valid_acc_list_type, label='Valid')\n","    plt.title('Type Accuracy vs. Epoch')\n","    plt.xlabel('Epoch')\n","    plt.ylabel('Accuracy')\n","    plt.legend()\n","    plt.show()\n","    plt.savefig(r\"C:\\Users\\Mark\\Desktop\\3rd year 1st term\\ECE324\\Project\\Transfer Learning\\Plot\\Type Accuracy vs. Epoch.png\")\n","\n","    #Plot State loss vs epoch\n","    plt.figure()\n","    plt.plot(epoch_num, train_loss_list_state, label='Train')\n","    plt.plot(epoch_num, valid_loss_list_state, label='Valid')\n","    plt.title('State Loss vs. Epoch')\n","    plt.xlabel('Epoch')\n","    plt.ylabel('Loss')\n","    plt.show()\n","    plt.savefig(r\"C:\\Users\\Mark\\Desktop\\3rd year 1st term\\ECE324\\Project\\Transfer Learning\\Plot\\State Loss vs. Epoch.png\")\n","\n","    #Plot TYPE Accuracy vs epoch\n","    plt.figure()\n","    plt.plot(epoch_num, train_acc_list_state, label='Train')\n","    plt.plot(epoch_num, valid_acc_list_state, label='Valid')\n","    plt.title('State Accuracy vs. Epoch')\n","    plt.xlabel('Epoch')\n","    plt.ylabel('Accuracy')\n","    plt.legend()\n","    plt.show()\n","    plt.savefig(r\"C:\\Users\\Mark\\Desktop\\3rd year 1st term\\ECE324\\Project\\Transfer Learning\\Plot\\State Accuracy vs. Epoch.png\")"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"JQrXbn3nsMCe"},"source":["Confusion Matrix and Classification Report Function"]},{"cell_type":"code","metadata":{"id":"B194cCnfq5YJ"},"source":["def confusion_classification_report(model,dataloader):\n","    predlist_state=torch.zeros(0,dtype=torch.long, device='cpu')\n","    lbllist_state=torch.zeros(0,dtype=torch.long, device='cpu')\n","    predlist_type=torch.zeros(0,dtype=torch.long, device='cpu')\n","    lbllist_type=torch.zeros(0,dtype=torch.long, device='cpu')\n","    predlist=torch.zeros(0,dtype=torch.long, device='cpu')\n","    lbllist=torch.zeros(0,dtype=torch.long, device='cpu')\n","    \n","    \n","    with torch.no_grad():\n","        for i, data in enumerate(dataloader):\n","            inputs, label = data[0].to(device), data[1].to(device)\n","            label_state = torch.zeros(len(label)).to(device)\n","            label_type = torch.zeros(len(label)).to(device)\n","            for k in range(len(label)):\n","                label_state[k] = relabel_state(label[k])\n","                label_type[k] = relabel_type(label[k])\n","                \n","            \n","            predictions_state, predictions_type = model(inputs)\n","            _, predicted_state = torch.max(predictions_state.data, 1)\n","            _,predicted_type = torch.max(predictions_type.data, 1)\n","    \n","            # Append batch prediction results\n","            predlist_state=torch.cat([predlist_state,predicted_state.view(-1).cpu()])\n","            lbllist_state=torch.cat([lbllist_state,label_state.view(-1).cpu()])\n","            predlist_type=torch.cat([predlist_type,predicted_type.view(-1).cpu()])\n","            lbllist_type=torch.cat([lbllist_type,label_type.view(-1).cpu()])\n","            \n","            predicted = torch.zeros(len(label)).to(device)\n","            # 15 label confusion matrix\n","            for k in range(len(predicted_state)):\n","                predicted[k] = recombine_prediction(predicted_state[k],predicted_type[k])\n","            predlist=torch.cat([predlist,predicted.view(-1).cpu()])\n","            lbllist=torch.cat([lbllist,label.view(-1).cpu()])\n","            \n","    \n","    \n","    \n","    \n","    \n","    \n","    # Confusion matrix and classification report\n","    conf_mat_state=confusion_matrix(lbllist_state.numpy(), predlist_state.numpy())\n","    conf_mat_type=confusion_matrix(lbllist_type.numpy(), predlist_type.numpy())\n","    conf_mat=confusion_matrix(lbllist.numpy(), predlist.numpy())\n","    class_report_state = classification_report(lbllist_state.numpy(), predlist_state.numpy())\n","    class_report_type = classification_report(lbllist_type.numpy(), predlist_type.numpy())\n","    class_report = classification_report(lbllist.numpy(), predlist.numpy())\n","    \n","    # print(conf_mat)\n","    # print(conf_mat_state)\n","    # print(conf_mat_type)\n","    print(class_report_state)\n","    print(class_report_type)\n","    print(class_report)\n","    \n","    # Per-class accuracy\n","    class_accuracy=100*conf_mat.diagonal()/conf_mat.sum(1)\n","    print(class_accuracy)\n","    class_accuracy_state=100*conf_mat_state.diagonal()/conf_mat_state.sum(1)\n","    print(class_accuracy_state)\n","    class_accuracy_type=100*conf_mat_type.diagonal()/conf_mat_type.sum(1)\n","    print(class_accuracy_type)\n","    return conf_mat_state, conf_mat_type, conf_mat, class_report_state, class_report_type, class_report"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"SayMvfAkq8Kg"},"source":["model = torch.load(r\"C:\\Users\\Mark\\Desktop\\3rd year 1st term\\ECE324\\Project\\Transfer Learning\\Model\\complete.pt\")\n","if confusion:\n","    conf_mat_state, conf_mat_type, conf_mat, class_report_state, class_report_type, class_report = confusion_classification_report(model, valid_loader)\n","\n","    plt.figure()\n","    total_map = sns.heatmap(conf_mat,cmap=\"YlGnBu\", annot=True,fmt='g',square = True,xticklabels=['ri_A', 'ri_B', 'ri_O', 'ri_R', 'ri_S', 'ro_A', 'ro_B', 'ro_O', 'ro_R', 'ro_S', 'ur_A', 'ur_B', 'ur_O', 'ur_R', 'ur_S'], yticklabels=['ri_A', 'ri_B', 'ri_O', 'ri_R', 'ri_S', 'ro_A', 'ro_B', 'ro_O', 'ro_R', 'ro_S', 'ur_A', 'ur_B', 'ur_O', 'ur_R', 'ur_S'] )\n","    bottom, top = total_map.get_ylim()\n","    total_map.set_ylim(bottom + 0.5, top - 0.5)\n","    total_map.set_title('Total Confusion Matrix')\n","    total_map.set_ylabel('True Label')\n","    total_map.set_xlabel('Predicted Label')\n","    plt.show()\n","\n","\n","\n","\n","    plt.figure()\n","    state_map = sns.heatmap(conf_mat_state,cmap=\"YlGnBu\", annot=True,fmt='g',square = True,xticklabels=['Ripe', 'Rotten', 'Unripe'], yticklabels=['Ripe', 'Rotten', 'Unripe'] )\n","    bottom, top = state_map.get_ylim()\n","    state_map.set_ylim(bottom + 0.5, top - 0.5)\n","    state_map.set_title('State Confusion Matrix')\n","    state_map.set_ylabel('True Label')\n","    state_map.set_xlabel('Predicted Label')\n","    plt.show()\n","\n","    plt.figure()\n","    type_map = sns.heatmap(conf_mat_type,cmap=\"YlGnBu\", annot=True,fmt='g',square = True,xticklabels=['Apple','Banana','Orange','Raspberry','Strawberry'], yticklabels=['Apple','Banana','Orange','Raspberry','Strawberry'] )\n","    bottom, top = type_map.get_ylim()\n","    type_map.set_ylim(bottom + 0.5, top - 0.5)\n","    type_map.set_title('Type Confusion Matrix')\n","    type_map.set_ylabel('True Label')\n","    type_map.set_yticklabels(['Apple','Banana','Orange','Raspberry','Strawberry'], rotation = 45, ha=\"right\")\n","    type_map.set_xticklabels(['Apple','Banana','Orange','Raspberry','Strawberry'], rotation = 45, ha=\"right\")\n","    type_map.set_xlabel('Predicted Label')\n","    plt.tight_layout()\n","\n","    plt.show()\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"O3JIF50IsRGG"},"source":["Specialized Backround Dataset"]},{"cell_type":"code","metadata":{"id":"0aapFvn_q-Gk"},"source":["test_white =  torchvision.datasets.ImageFolder(r\"C:\\Users\\Mark\\Desktop\\3rd year 1st term\\ECE324\\Project\\Special Data\\test_white_bg\", transform=my_transform)\n","test_varied =  torchvision.datasets.ImageFolder(r\"C:\\Users\\Mark\\Desktop\\3rd year 1st term\\ECE324\\Project\\Special Data\\test_varied_bg\", transform=my_transform)\n","\n","test_white_loader = DataLoader(test_white, batch_size=4)\n","test_varied_loader = DataLoader(test_varied, batch_size=4)\n","\n","\n","running_loss = []\n","running_accuracy = []\n","running_loss_state = []\n","running_accuracy_state = []\n","running_loss_type = []\n","running_accuracy_type = []\n","for i, data in enumerate(test_white_loader):\n","        #get batch of data\n","    inputs, label = data[0].to(device), data[1].to(device)\n","\n","    label_state = torch.zeros(len(label)).to(device)\n","    label_type = torch.zeros(len(label)).to(device)\n","    for k in range(len(label)):\n","        label_state[k] = relabel_state(label[k])\n","        label_type[k] = relabel_type(label[k]) \n","\n","    #run model on validation batch\n","    predictions_state, predictions_type = model(inputs)\n","\n","        #compute loss\n","    batch_loss_state = loss_fnc(input=predictions_state.squeeze(), target=label_state.long())\n","    batch_loss_type = loss_fnc(input=predictions_type.squeeze(), target=label_type.long())\n","\n","    Overall_loss = batch_loss_state + batch_loss_type\n","        #evaluate\n","    _, predicted_state = torch.max(predictions_state.data, 1)\n","    _,predicted_type = torch.max(predictions_type.data, 1)\n","\n","    Acc_state = (label_state == predicted_state).sum().item() / 4\n","    Acc_type = (label_type == predicted_type).sum().item() / 4\n","    running_loss.append(Overall_loss.item())\n","    running_accuracy.append((Acc_state+Acc_type)/2)\n","\n","    running_loss_state.append(batch_loss_state.item())\n","    running_accuracy_state.append(Acc_state)\n","    running_loss_type.append(batch_loss_type.item())\n","    running_accuracy_type.append(Acc_type) \n","    \n","#Overall accuracy\n","whiteacc_ = sum(running_accuracy) / float(len(running_accuracy))\n","#state accuracy\n","whiteacc_state_ = sum(running_accuracy_state) / float(len(running_accuracy_state))\n","\n","#type accuracy\n","whiteAcc_type_ = sum(running_accuracy_type) / float(len(running_accuracy_type))\n","\n","#             lists for VALIDATION losses\n","#Overall Loss\n","white_loss_ = sum(running_loss) / float(len(running_loss))\n","\n","#State Loss\n","white_loss_state_ = sum(running_loss_state) / float(len(running_loss_state))\n","\n","#Type Loss\n","white_loss_type_ = sum(running_loss_type) / float(len(running_loss_type))\n","\n","print(whiteacc_)\n","print(white_loss_)\n","\n","\n","\n","running_loss = []\n","running_accuracy = []\n","running_loss_state = []\n","running_accuracy_state = []\n","running_loss_type = []\n","running_accuracy_type = []\n","for i, data in enumerate(test_varied_loader):\n","        #get batch of data\n","    inputs, label = data[0].to(device), data[1].to(device)\n","\n","    label_state = torch.zeros(len(label)).to(device)\n","    label_type = torch.zeros(len(label)).to(device)\n","    for k in range(len(label)):\n","        label_state[k] = relabel_state(label[k])\n","        label_type[k] = relabel_type(label[k]) \n","\n","    #run model on validation batch\n","    predictions_state, predictions_type = model(inputs)\n","\n","        #compute loss\n","    batch_loss_state = loss_fnc(input=predictions_state.squeeze(), target=label_state.long())\n","    batch_loss_type = loss_fnc(input=predictions_type.squeeze(), target=label_type.long())\n","\n","    Overall_loss = batch_loss_state + batch_loss_type\n","        #evaluate\n","    _, predicted_state = torch.max(predictions_state.data, 1)\n","    _,predicted_type = torch.max(predictions_type.data, 1)\n","\n","    Acc_state = (label_state == predicted_state).sum().item() / 4\n","    Acc_type = (label_type == predicted_type).sum().item() / 4\n","    running_loss.append(Overall_loss.item())\n","    running_accuracy.append((Acc_state+Acc_type)/2)\n","\n","    running_loss_state.append(batch_loss_state.item())\n","    running_accuracy_state.append(Acc_state)\n","    running_loss_type.append(batch_loss_type.item())\n","    running_accuracy_type.append(Acc_type) \n","    \n","#Overall accuracy\n","variedacc_ = sum(running_accuracy) / float(len(running_accuracy))\n","#state accuracy\n","variedacc_state_ = sum(running_accuracy_state) / float(len(running_accuracy_state))\n","\n","#type accuracy\n","variedAcc_type_ = sum(running_accuracy_type) / float(len(running_accuracy_type))\n","\n","#             lists for VALIDATION losses\n","#Overall Loss\n","varied_loss_ = sum(running_loss) / float(len(running_loss))\n","\n","#State Loss\n","varied_loss_state_ = sum(running_loss_state) / float(len(running_loss_state))\n","\n","#Type Loss\n","varied_loss_type_ = sum(running_loss_type) / float(len(running_loss_type))\n","\n","print(variedacc_)\n","print(varied_loss_)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"A2x0f1eHsVTo"},"source":["Test DataSet"]},{"cell_type":"code","metadata":{"id":"beGNZ4oprCTY"},"source":["running_loss = []\n","running_accuracy = []\n","running_loss_state = []\n","running_accuracy_state = []\n","running_loss_type = []\n","running_accuracy_type = []\n","for i, data in enumerate(test_loader):\n","        #get batch of data\n","    inputs, label = data[0].to(device), data[1].to(device)\n","\n","    label_state = torch.zeros(len(label)).to(device)\n","    label_type = torch.zeros(len(label)).to(device)\n","    for k in range(len(label)):\n","        label_state[k] = relabel_state(label[k])\n","        label_type[k] = relabel_type(label[k]) \n","\n","    #run model on validation batch\n","    predictions_state, predictions_type = model(inputs)\n","\n","        #compute loss\n","    batch_loss_state = loss_fnc(input=predictions_state.squeeze(), target=label_state.long())\n","    batch_loss_type = loss_fnc(input=predictions_type.squeeze(), target=label_type.long())\n","\n","    Overall_loss = batch_loss_state + batch_loss_type\n","        #evaluate\n","    _, predicted_state = torch.max(predictions_state.data, 1)\n","    _,predicted_type = torch.max(predictions_type.data, 1)\n","\n","    Acc_state = (label_state == predicted_state).sum().item() / 64\n","    Acc_type = (label_type == predicted_type).sum().item() / 64\n","    running_loss.append(Overall_loss.item())\n","    running_accuracy.append((Acc_state+Acc_type)/2)\n","\n","    running_loss_state.append(batch_loss_state.item())\n","    running_accuracy_state.append(Acc_state)\n","    running_loss_type.append(batch_loss_type.item())\n","    running_accuracy_type.append(Acc_type) \n","    \n","#Overall accuracy\n","testacc_ = sum(running_accuracy) / float(len(running_accuracy))\n","#state accuracy\n","testacc_state_ = sum(running_accuracy_state) / float(len(running_accuracy_state))\n","\n","#type accuracy\n","testAcc_type_ = sum(running_accuracy_type) / float(len(running_accuracy_type))\n","\n","#             lists for VALIDATION losses\n","#Overall Loss\n","test_loss_ = sum(running_loss) / float(len(running_loss))\n","\n","#State Loss\n","test_loss_state_ = sum(running_loss_state) / float(len(running_loss_state))\n","\n","#Type Loss\n","test_loss_type_ = sum(running_loss_type) / float(len(running_loss_type))\n","\n","print(testacc_)\n","print(testacc_state_)\n","print(testAcc_type_)\n","print(test_loss_)"],"execution_count":null,"outputs":[]}]}