{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"TransferLearning.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"code","metadata":{"id":"kvMLiZpxDWXE"},"source":["import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","import numpy as np\n","import torchvision\n","from torchvision import datasets, models, transforms\n","import matplotlib.pyplot as plt\n","import os\n","import copy\n","import numpy as np                                              \n","import pandas as pd                                             \n","import matplotlib.pyplot as plt                                 \n","from sklearn.preprocessing import LabelEncoder, OneHotEncoder               \n","import torchvision.transforms as transforms              \n","from torch.utils.data import SubsetRandomSampler \n","import torch.utils.data as data   \n","from torchsummary import summary\n","import torch.nn.functional as F\n","from time import time"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"JSaqmCmUJknC","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1605653301836,"user_tz":300,"elapsed":15438,"user":{"displayName":"pwhobbit","photoUrl":"","userId":"08214086800411424161"}},"outputId":"061f7fb0-509b-4084-faac-abf919001f0b"},"source":["from google.colab import drive\n","\n","drive.mount('/content/drive', force_remount=True)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Mounted at /content/drive\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"U54vzEmqED29"},"source":["from torchvision import models\n","from torch import nn\n","vgg = models.vgg16_bn(pretrained=True)\n","for param in vgg.features.parameters():\n","    param.requires_grad = False"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"3ARVDIyoGaKK"},"source":["# Unfreeze upto 34\n","# ct = 0\n","# for child in vgg.children():\n","#     ct += 1\n","#     if ct < 34:\n","#         for param in child.parameters():\n","#             param.requires_grad = False"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"6uOp7PuYGXvg"},"source":["torch.cuda.empty_cache()\n","device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"NZ2zFlBHKmXF"},"source":["#Convolutional Layer with Batch Norm\n","vgg.features[43] = nn.Sequential(vgg.features[43],nn.Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)), nn.BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True),\n","                nn.ReLU(inplace=True),nn.MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False))\n","#                 \n","#Two Convolutional Layer with Batch Norm\n","# vgg.features[43] = nn.Sequential(vgg.features[43],nn.Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)), nn.BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True),\n","#                 nn.ReLU(inplace=True),nn.MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False),nn.Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)), nn.BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True),\n","#                 nn.ReLU(inplace=True),nn.MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False))\n","                \n","print(nn.Sequential(*list(vgg.children())[:-1]))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"AgUhZjKmb_O5"},"source":["#Loading Data \n","my_transform = transforms.Compose([\n","  transforms.ToTensor(),\n","  transforms.ToPILImage(),\n","  transforms.Resize((224,224)),\n","  transforms.ToTensor(),\n","  transforms.Normalize(mean = [0.66445047,0.55465436,0.447036], std = [0.321551,0.33547384,0.3524585])\n","  ])\n","dataset =  torchvision.datasets.ImageFolder(r\"C:\\Users\\Mark\\Desktop\\3rd year 1st term\\ECE324\\Project\\Clean Data\\Clean Data\", transform=my_transform)\n","\n","train_data, valid_data, test_data = torch.utils.data.random_split(dataset, [9600,2400,3000], generator=torch.Generator().manual_seed(0))\n","\n","train_loader = data.DataLoader(train_data, batch_size=64,shuffle= True)                  \n","valid_loader = data.DataLoader(valid_data, batch_size=64,shuffle= False)\n","test_loader = data.DataLoader(test_data, batch_size=64,shuffle= False)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ItAyjdHpY6Nw"},"source":["# Relabel Functions\n","def relabel_state(argument):\n","    switcher = {\n","        0: 0,\n","        1: 0,\n","        2: 0,\n","        3: 0,\n","        4: 0,\n","        5: 1,\n","        6: 1,\n","        7: 1,\n","        8: 1,\n","        9: 1,\n","        10:2,\n","        11:2,\n","        12:2,\n","        13:2,\n","        14:2}\n","    return switcher[argument.item()]\n","\n","def relabel_type(argument):\n","    switcher = {\n","        0: 0,\n","        1: 1,\n","        2: 2,\n","        3: 3,\n","        4: 4,\n","        5: 0,\n","        6: 1,\n","        7: 2,\n","        8: 3,\n","        9: 4,\n","        10:0,\n","        11:1,\n","        12:2,\n","        13:3,\n","        14:4}\n","    return switcher[argument.item()]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"drwt0GEkEHtg"},"source":["#Transfer Learning Multi Head\n","class MyVgg(nn.Module):\n","    def __init__(self,originalmodel):\n","        super(MyVgg,self).__init__()\n","        vgg = originalmodel\n","        # Here you get the bottleneck/feature extractor\n","        self.vgg_feature_extractor = nn.Sequential(*list(vgg.children())[:-1])\n","        self.classifier1 = nn.Sequential(\n","            nn.Linear(512 * 7 * 7, 4096),\n","            nn.ReLU(True),\n","            nn.Linear(4096, 4096),\n","            nn.ReLU(True),\n","            nn.Linear(4096, 3),\n","            nn.Sigmoid()\n","        )\n","        self.classifier2 = nn.Sequential(\n","            nn.Linear(512 * 7 * 7, 4096),\n","            nn.ReLU(True),\n","            nn.Linear(4096, 4096),\n","            nn.ReLU(True),\n","            nn.Linear(4096, 5),\n","            nn.Sigmoid()\n","                            )\n","\n","    # Set your own forward pass\n","    def forward(self, img, extra_info=None):\n","        \n","        x = self.vgg_feature_extractor(img)\n","        x = x.view(x.size(0), -1)\n","        x1 = self.classifier1(x)\n","        x2 = self.classifier2(x)\n","\n","        return x1, x2"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"7EMgYjj3GpK6"},"source":["# Extra MLP Layer for State\n","# class MyVggextra(nn.Module):\n","#     def __init__(self,originalmodel):\n","#         super(MyVggextra,self).__init__()\n","#         vgg = originalmodel\n","#         # Here you get the bottleneck/feature extractor\n","#         self.vgg_feature_extractor = nn.Sequential(*list(vgg.children())[:-1])\n","#         self.classifier1 = nn.Sequential(\n","#             nn.Linear(512 * 7 * 7, 4096),\n","#             nn.ReLU(True),\n","#             nn.Linear(4096, 4096),\n","#             nn.ReLU(True),\n","#             nn.Linear(4096, 4096),\n","#             nn.ReLU(True),\n","#             nn.Linear(4096, 4096),\n","#             nn.ReLU(True),\n","#             nn.Linear(4096, 4096),\n","#             nn.ReLU(True),\n","#             nn.Linear(4096, 3),\n","#             nn.Sigmoid()\n","#         )\n","#         self.classifier2 = nn.Sequential(\n","#             nn.Linear(512 * 7 * 7, 4096),\n","#             nn.ReLU(True),\n","#             nn.Linear(4096, 4096),\n","#             nn.ReLU(True),\n","#             nn.Linear(4096, 5),\n","#             nn.Sigmoid()\n","#                             )\n","# \n","#     # Set your own forward pass\n","#     def forward(self, img, extra_info=None):\n","#         \n","#         x = self.vgg_feature_extractor(img)\n","#         x = x.view(x.size(0), -1)\n","#         x1 = self.classifier1(x)\n","#         x2 = self.classifier2(x)\n","# \n","#         return x1, x2"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"T_Vs1SReGvMF"},"source":["# Convolutional Layer on each head\n","# class MyVggeach(nn.Module):\n","#     def __init__(self,originalmodel):\n","#         super(MyVggeach,self).__init__()\n","#         vgg = originalmodel\n","#         # Here you get the bottleneck/feature extractor\n","#         self.vgg_feature_extractor = nn.Sequential(*list(vgg.children())[:-1])\n","#         self.convlayer1 = nn.Sequential(nn.Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)), nn.BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True),\n","#                 nn.ReLU(inplace=True),nn.MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False))\n","#         self.convlayer2 = nn.Sequential(nn.Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)), nn.BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True),\n","#                 nn.ReLU(inplace=True),nn.MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False),nn.Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)), nn.BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True),\n","#                 nn.ReLU(inplace=True),nn.MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False))\n","#         self.classifier1 = nn.Sequential(\n","#             nn.Linear(512 * 7 * 7, 4096),\n","#             nn.ReLU(True),\n","#             nn.Linear(4096, 4096),\n","#             nn.ReLU(True),\n","#             nn.Linear(4096, 4096),\n","#             nn.ReLU(True),\n","#             nn.Linear(4096, 4096),\n","#             nn.ReLU(True),\n","#             nn.Linear(4096, 4096),\n","#             nn.ReLU(True),\n","#             nn.Linear(4096, 3),\n","#             nn.Sigmoid()\n","#         )\n","#         self.classifier2 = nn.Sequential(\n","#             nn.Linear(512 * 7 * 7, 4096),\n","#             nn.ReLU(True),\n","#             nn.Linear(4096, 4096),\n","#             nn.ReLU(True),\n","#             nn.Linear(4096, 5),\n","#             nn.Sigmoid()\n","#                             )\n","# \n","#     # Set your own forward pass\n","#     def forward(self, img, extra_info=None):\n","#         \n","#         x = self.vgg_feature_extractor(img)\n","#         x1= self.convlayer1(x)\n","#         x2= self.convlayer2(x)\n","#         x1 = x.view(x1.size(0), -1)\n","#         x2 = x.view(x2.size(0), -1)\n","#         #torch.Size([64, 25088])\n","#         x1 = self.classifier1(x1)\n","#         x2 = self.classifier2(x2)\n","# \n","#         return x1, x2"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"gyBWXzBqZxwC"},"source":["#MODEL ARGUMENTS\n","lr = 0.01\n","epochs = 10\n","eval_every = 60\n","model = MyVgg(vgg)\n","model = model.to(device)\n","\n","optimizer = torch.optim.SGD(model.parameters(), lr=lr)\n","loss_fnc= torch.nn.CrossEntropyLoss()\n","loss_fnc =loss_fnc.to(device)\n","\n","\n","#OUTPUT DATA ARGUMENTS\n","seed = 1\n","torch.manual_seed(seed)\n","save= True\n","confusion=True\n","plot = True\n","statistics = True"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"KYYqMqYDaANv"},"source":["train_loss_list = []\n","train_acc_list = []\n","train_loss_list_state = []\n","train_acc_list_state = []\n","train_loss_list_type = []\n","train_acc_list_type = []\n","\n","valid_loss_list = []\n","valid_acc_list = []\n","valid_loss_list_state = []\n","valid_acc_list_state = []\n","valid_loss_list_type = []\n","valid_acc_list_type = []\n","\n","epoch_num = []         #Epoch number\n","start = time()\n","for e in range(epochs):\n","    print(\"Time:    \", time() - start)\n","    running_loss = []\n","    running_accuracy = []\n","    running_loss_state = []\n","    running_accuracy_state = []\n","    running_loss_type = []\n","    running_accuracy_type = []\n","\n","    running_valid_loss = []\n","    running_valid_accuracy = []\n","    running_valid_loss_state = []\n","    running_valid_accuracy_state = []\n","    running_valid_loss_type = []\n","    running_valid_accuracy_type = []\n","    model.train()\n","  #               LOOKING AT TRAINING DATA\n","\n","    for i, data in enumerate(train_loader):\n","\n","            #get batch of data\n","        inputs, label = data[0].to(device), data[1].to(device)\n","    \n","        label_state = torch.zeros(len(label)).to(device)\n","        label_type = torch.zeros(len(label)).to(device)\n","        for k in range(len(label)):\n","          label_state[k] = relabel_state(label[k])\n","          label_type[k] = relabel_type(label[k])\n","        \n","        # Setting gradients to zero and running model on batch\n","        optimizer.zero_grad()\n","        predictions_state, predictions_type = model(inputs)\n","\n","        # Computing loss based on Cross Entropy\n","        batch_loss_state = loss_fnc(input=predictions_state.squeeze(), target=label_state.long())\n","        batch_loss_type = loss_fnc(input=predictions_type.squeeze(), target=label_type.long())\n","\n","        # Combining losses and calculating gradients\n","        Overall_loss = batch_loss_state + batch_loss_type\n","        Overall_loss.backward()\n","        optimizer.step()\n","        # Evaluating Training Acc\n","        _, predicted_state = torch.max(predictions_state.data, 1)\n","        _,predicted_type = torch.max(predictions_type.data, 1)\n","        trainAcc_state = (label_state == predicted_state).sum().item() / 64\n","        trainAcc_type = (label_type == predicted_type).sum().item() / 64\n","\n","        if i % eval_every == 0:\n","           print(\"epoch: {} {} |||   loss type:   {}  trainAcc type:   {} |||  loss state {}  trainAcc state:  {}  \".format(e+1,i, batch_loss_type, trainAcc_type, batch_loss_state, trainAcc_state))\n","\n","        running_loss.append(Overall_loss.item())\n","        running_accuracy.append((trainAcc_state+trainAcc_type)/2)\n","        running_loss_state.append(batch_loss_state.item())\n","        running_accuracy_state.append(trainAcc_state)\n","        running_loss_type.append(batch_loss_type.item())\n","        running_accuracy_type.append(trainAcc_type)\n","\n","        del Overall_loss\n","        del batch_loss_state\n","        del batch_loss_type\n","        del label_state\n","        del label_type\n","        del predictions_state\n","        del predictions_type\n","\n","    \n","    # \n","    model.eval()\n","    for j, data in enumerate(valid_loader):\n","            #get batch of data\n","        inputs, label = data[0].to(device), data[1].to(device)\n","    \n","        vlabel_state = torch.zeros(len(label)).to(device)\n","        vlabel_type = torch.zeros(len(label)).to(device)\n","        for k in range(len(label)):\n","            vlabel_state[k] = relabel_state(label[k])\n","            vlabel_type[k] = relabel_type(label[k]) \n","    \n","        #run model on validation batch\n","        predictions_state_v, predictions_type_v = model(inputs)\n","    \n","            #compute loss\n","        batch_valid_loss_state = loss_fnc(input=predictions_state_v.squeeze(), target=vlabel_state.long())\n","        batch_valid_loss_type = loss_fnc(input=predictions_type_v.squeeze(), target=vlabel_type.long())\n","    \n","        Overall_loss_v = batch_valid_loss_state + batch_valid_loss_type\n","            #evaluate\n","        _, predicted_state_v = torch.max(predictions_state_v.data, 1)\n","        _,predicted_type_v = torch.max(predictions_type_v.data, 1)\n","    \n","        validAcc_state = (vlabel_state == predicted_state_v).sum().item() / 64\n","        validAcc_type = (vlabel_type == predicted_type_v).sum().item() / 64\n","        \n","        if j % eval_every == 0:\n","            print(\"epoch: {} |||   vloss type:   {}  validAcc type:   {} |||  vloss state {}  validAcc state:  {}  \".format(e+1, batch_valid_loss_type, validAcc_type, batch_valid_loss_state,validAcc_state))\n","        \n","        running_valid_loss.append(Overall_loss_v.item())\n","        running_valid_accuracy.append((validAcc_state+validAcc_type)/2)\n","    \n","        running_valid_loss_state.append(batch_valid_loss_state.item())\n","        running_valid_accuracy_state.append(validAcc_state)\n","        running_valid_loss_type.append(batch_valid_loss_type.item())\n","        running_valid_accuracy_type.append(validAcc_type) \n","        del Overall_loss_v\n","        del batch_valid_loss_state\n","        del batch_valid_loss_type\n","        del vlabel_state\n","        del vlabel_type\n","        del predicted_state_v\n","        del predicted_type_v\n","    #     \n","    #Overall accuracy\n","    trainacc_ = sum(running_accuracy) / float(len(running_accuracy))\n","    train_acc_list.append(trainacc_)\n","    #state accuracy\n","    trainacc_state_ = sum(running_accuracy_state) / float(len(running_accuracy_state))\n","    train_acc_list_state.append(trainacc_state_)\n","    #type accuracy\n","    trainAcc_type_ = sum(running_accuracy_type) / float(len(running_accuracy_type))\n","    train_acc_list_type.append(trainAcc_type_) \n","#     \n","    #             lists for TRAINING losses\n","    #Overall Loss\n","    loss_ = sum(running_loss) / float(len(running_loss))\n","    train_loss_list.append(loss_)\n","    #State Loss\n","    loss_state_ = sum(running_loss_state) / float(len(running_loss_state))\n","    train_loss_list_state.append(loss_state_)\n","    #Type Loss\n","    loss_type_ = sum(running_loss_type) / float(len(running_loss_type))\n","    train_loss_list_type.append(loss_type_) \n","        \n","    #------------------------------------------------------------------------------------\n","    #             lists for VALIDATION accuracy\n","    #Overall accuracy\n","    validacc_ = sum(running_valid_accuracy) / float(len(running_valid_accuracy))\n","    valid_acc_list.append(validacc_)\n","    #state accuracy\n","    validacc_state_ = sum(running_valid_accuracy_state) / float(len(running_valid_accuracy_state))\n","    valid_acc_list_state.append(validacc_state_)\n","    #type accuracy\n","    validAcc_type_ = sum(running_valid_accuracy_type) / float(len(running_valid_accuracy_type))\n","    valid_acc_list_type.append(validAcc_type_)\n","    \n","    #             lists for VALIDATION losses\n","    #Overall Loss\n","    valid_loss_ = sum(running_valid_loss) / float(len(running_valid_loss))\n","    valid_loss_list.append(valid_loss_)\n","    #State Loss\n","    valid_loss_state_ = sum(running_valid_loss_state) / float(len(running_valid_loss_state))\n","    valid_loss_list_state.append(valid_loss_state_)\n","    #Type Loss\n","    valid_loss_type_ = sum(running_valid_loss_type) / float(len(running_valid_loss_type))\n","    valid_loss_list_type.append(valid_loss_type_)\n","    \n","    epoch_num.append(e)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"9fqKGVk5EJi2"},"source":["#Plot overall loss vs epoch\n","plt.figure()\n","plt.plot(epoch_num, train_loss_list, label='Train')\n","plt.plot(epoch_num, valid_loss_list, label='Valid')\n","plt.title('Overall Loss vs. Epoch')\n","plt.xlabel('Epoch')\n","plt.ylabel('Loss')\n","plt.show()\n","plt.savefig(r\"C:\\Users\\Mark\\Desktop\\3rd year 1st term\\ECE324\\Project\\Transfer Learning\\Plot\\Overall Loss vs. Epoch.png\")\n","\n","#Plot overall Accuracy vs epoch\n","plt.figure()\n","plt.plot(epoch_num, train_acc_list, label='Train')\n","plt.plot(epoch_num, valid_acc_list, label='Valid')\n","plt.title('Overall Accuracy vs. Epoch')\n","plt.xlabel('Epoch')\n","plt.ylabel('Accuracy')\n","plt.legend()\n","plt.show()\n","plt.savefig(r\"C:\\Users\\Mark\\Desktop\\3rd year 1st term\\ECE324\\Project\\Transfer Learning\\Plot\\Overall Accuracy vs. Epoch.png\")\n","\n","#Plot TYPE loss vs epoch\n","plt.figure()    \n","plt.plot(epoch_num, train_loss_list_type, label='Train')\n","plt.plot(epoch_num, valid_loss_list_type, label='Valid')\n","plt.title('Type Loss vs. Epoch')\n","plt.xlabel('Epoch')\n","plt.ylabel('Loss')\n","plt.show()\n","plt.savefig(r\"C:\\Users\\Mark\\Desktop\\3rd year 1st term\\ECE324\\Project\\Transfer Learning\\Plot\\Type Loss vs. Epoch.png\")\n","\n","#Plot TYPE Accuracy vs epoch\n","plt.figure()\n","plt.plot(epoch_num, train_acc_list_type, label='Train')\n","plt.plot(epoch_num, valid_acc_list_type, label='Valid')\n","plt.title('Type Accuracy vs. Epoch')\n","plt.xlabel('Epoch')\n","plt.ylabel('Accuracy')\n","plt.legend()\n","plt.show()\n","plt.savefig(r\"C:\\Users\\Mark\\Desktop\\3rd year 1st term\\ECE324\\Project\\Transfer Learning\\Plot\\Type Accuracy vs. Epoch.png\")\n","\n","#Plot State loss vs epoch\n","plt.figure()\n","plt.plot(epoch_num, train_loss_list_state, label='Train')\n","plt.plot(epoch_num, valid_loss_list_state, label='Valid')\n","plt.title('State Loss vs. Epoch')\n","plt.xlabel('Epoch')\n","plt.ylabel('Loss')\n","plt.show()\n","plt.savefig(r\"C:\\Users\\Mark\\Desktop\\3rd year 1st term\\ECE324\\Project\\Transfer Learning\\Plot\\State Loss vs. Epoch.png\")\n","\n","#Plot TYPE Accuracy vs epoch\n","plt.figure()\n","plt.plot(epoch_num, train_acc_list_state, label='Train')\n","plt.plot(epoch_num, valid_acc_list_state, label='Valid')\n","plt.title('State Accuracy vs. Epoch')\n","plt.xlabel('Epoch')\n","plt.ylabel('Accuracy')\n","plt.legend()\n","plt.show()\n","plt.savefig(r\"C:\\Users\\Mark\\Desktop\\3rd year 1st term\\ECE324\\Project\\Transfer Learning\\Plot\\State Accuracy vs. Epoch.png\")"],"execution_count":null,"outputs":[]}]}
